{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cross validation\n",
    "* Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import sklearn.datasets as ds\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation is one of the most important procedure, when you select the model or tune the parameters. There are several possibilities to perform cross validation in the ```scikit-learn```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import LeaveOneOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to build cross validation splits is to use the method ```train_test_split```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boston = ds.load_boston()\n",
    "X = boston.data\n",
    "y = boston.target/50.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.33, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 339\n",
      "Number of validation examples: 167\n"
     ]
    }
   ],
   "source": [
    "print 'Number of training examples: ' + str(X_train.shape[0])\n",
    "print 'Number of validation examples: ' + str(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Another way is to use K-fold or leave-one-out cross validation procedures. Notice that we use the method ```StratifiedKFold``` that guarantees that the distribution of target variable stays the same for different folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 323\n",
      "Number of validation examples: 183\n",
      "Number of training examples: 332\n",
      "Number of validation examples: 174\n",
      "Number of training examples: 357\n",
      "Number of validation examples: 149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/sklearn/cross_validation.py:516: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.\n",
      "  % (min_labels, self.n_folds)), Warning)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(y, n_folds=3, shuffle=True, random_state=21387)\n",
    "for train_index, test_index in skf:\n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]    \n",
    "    print 'Number of training examples: ' + str(X_train.shape[0])\n",
    "    print 'Number of validation examples: ' + str(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the method ```LeaveOneOut``` we can perform leave-one-out cross validation procedure in similar way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we show how regularization procedure affects the weights of the linear regression model. We generate data with 10 features with 6 out of 10 informative features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = ds.make_regression(n_samples=100, \n",
    "                         n_features=10, \n",
    "                         n_informative=6, \n",
    "                         noise=1.0, \n",
    "                         bias=0, \n",
    "                         random_state=2016)\n",
    "y = y/10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different types of regularizations are implemented in the ```scikit-learn``` package. We will use ```Lasso``` class that implements L1 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us observe the model coefficients with respect to different values of the regularization parameter $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 0.0:\n",
      "[ -5.52107945e-03  -3.67860089e-03   7.64629577e+00   1.73280696e-01\n",
      "   2.82826703e-01   2.28833747e+00   5.71482110e-03   4.49741291e+00\n",
      "   5.56384364e+00   1.73914964e-03]\n",
      "***************\n",
      "alpha = 0.1:\n",
      "[ 0.          0.          7.51109466  0.09366317  0.14180965  2.18308828\n",
      " -0.          4.39417767  5.4250924  -0.        ]\n",
      "***************\n",
      "alpha = 0.5:\n",
      "[ 0.          0.          7.00822133  0.          0.          1.77766907\n",
      " -0.          3.9454249   4.87546516 -0.        ]\n",
      "***************\n",
      "alpha = 1.0:\n",
      "[ 0.          0.          6.39221165  0.          0.          1.27978247\n",
      " -0.          3.36381678  4.19028074 -0.        ]\n",
      "***************\n",
      "alpha = 10.0:\n",
      "[ 0.  0.  0.  0. -0.  0.  0.  0.  0. -0.]\n",
      "***************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/ipykernel/__main__.py:4: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "/Library/Python/2.7/site-packages/sklearn/linear_model/coordinate_descent.py:454: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "for a in [0.0, 0.1, 0.5, 1.0, 10.0]:\n",
    "    print 'alpha = ' + str(a) + ':'\n",
    "    model = Lasso(alpha=a, normalize=False, max_iter=1000000)\n",
    "    model.fit(X,y)\n",
    "    print model.coef_\n",
    "    print '***************'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
